---
layout: post
title: FFMpeg拉流过程
date: 2023-02-03
tag: iOS
---
###  FFMpeg拉流过程

<img src="/images/posts/FFMpeg/FFMpeg01.png" > 
图来自网络

<img src="/images/posts/FFMpeg/FFMpeg02.png" > 
图来自网络


```
//
//  YQMediaPlayer.m
//  YQFFmpeg
//
//  Created by iWolf on 2022/8/4.
//

#import "YQMediaPlayer.h"

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <libavcodec/avcodec.h>
//#include <libavdevice/avdevice.h>
#include <libavfilter/avfilter.h>
#include <libavformat/avformat.h>
#include <libavutil/avutil.h>
#include <libavutil/time.h>
#include <libswscale/swscale.h>
#include <libswresample/swresample.h>
#include <libavfilter/buffersrc.h>
#include <libavfilter/buffersink.h>
#include <libavutil/imgutils.h>
#import "ReactiveObjC.h"

@interface YQMediaPlayer(){
    AVFormatContext* format_ctx;//AVFormatContext是一个贯穿始终的数据结构，很多函数都要用到它作为参数。它是FFMPEG解封装（flv，mp4，rmvb，avi）功能的结构体
    AVCodecContext *pAVCodecContext_video;//AVCodecContext 解码器上下文 中很多的参数是编码的时候使用的，而不是解码的时候使用的。
    AVCodecParserContext *pAVCodeParseContext_video;//描述一个解码后的流的属性
    AVFrame *pAVFrame_video;
    AVFrame *pAVFrameRGB32_video;
    u_char *outBuffer;
    struct SwsContext *pSwsContext_video;
    
}
@property(nonatomic,strong)NSString * urlPath;
@end

@implementation YQMediaPlayer

-(id)init{
    self=[super init];
    if (self) {
        
    }
    return self;
}

#pragma - mark 设置播放Options 属性
-(AVDictionary *)setFFMOptions{
    
    AVDictionary *options = NULL;
            //2021.05.28 新增：强制使用tcp，udp在1080p下会丢包导致花屏
            //av_dict_set(&options, "max_delay", "350000", 0);
            //2021.05.28 修改：设置udp的接收缓冲 视频缓冲区大小
            //av_dict_set(&options, "buffer_size", "8388608", 0);
            av_dict_set(&options, "framedrop", "1", 0);
            av_dict_set(&options, "stimeout", "300000", 0);//设置超时3秒
            //无缓存，解码时有效
            av_dict_set(&options, "fflags", "nobuffer", 0);
    
            av_dict_set(&options, "probesize", "2048", 0);
            av_dict_set(&options, "max_analyze_duration", "10", 0);

            //解决find_stream_info函数阻塞时间太长
            /*  if (strcmp(playerState->url, "rtsp://192.168.1.1/liveRTSP/av4") == 0) {
                  
              }*/
            av_dict_set(&options, "probesize", "50000", 0);
            av_dict_set(&options, "max_analyze_duration", "1000000", 0);
            // 设置rtmp/rtsp的超时值
    
            if ([self.urlPath hasPrefix:@"rtmp"] || [self.urlPath hasPrefix:@"rtsp"]){
                av_dict_set(&options, "rtsp_transport", "tcp", 0); //设置tcp
                av_dict_set(&options, "stimeout", "2000000", 0);//rtsp设置超时2秒
            }else{
                av_dict_set(&options, "timeout", "2000000", 0);//http设置超时2秒
            }
    
    ////需要修改options值
//    av_dict_set(&options, "tune", "zerolatency", 0);
//    av_dict_set(&options, "fflags", "nobuffer", 0);
//    av_dict_set(&options, "buffer_size", "1024000", 0);
//    av_dict_set(&options, "fflags", "nobuffer", 0);
//    av_dict_set(&options, "max_delay", "500000", 0);//设置最大时延
//
//    if ([self.urlPath hasPrefix:@"rtmp"] || [self.urlPath hasPrefix:@"rtsp"]){
//        av_dict_set(&options, "rtsp_transport", "tcp", 0); //设置tcp
//        av_dict_set(&options, "stimeout", "2000000", 0);//rtsp设置超时2秒
//    }else{
//        av_dict_set(&options, "timeout", "2000000", 0);//http设置超时2秒
//    }
//    NSLog(@"直播打开文件的超时时间：%d",self.maxTimeOut);
    
//    const char *timeOut=[@"3000000" UTF8String];
//    AVDictionary* options=NULL;
//
//    if ([self.urlPath hasPrefix:@"http"]) {
//        av_dict_set(&options, "timeout", timeOut, 0);//设置超时3秒
//    }else{
//        av_dict_set(&options, "rtsp_transport", "tcp", 0);
//        av_dict_set(&options, "stimeout", timeOut, 0); // milliseconds
//    }
//    av_dict_set(&options, "probesize", "4096", 0);
//    av_dict_set(&options, "max_cached_duration", "30", 0);
//    av_dict_set(&options, "fflags", "nobuffer", 0);
    
//    av_dict_set(&options, "framedrop", "1", 0);
//    //2021.05.28 修改：probesize：探测长度，可以通过播放器设置，默认探测长度5000000字节
//    av_dict_set(&options, "probesize", "4096", 0);
//    av_dict_set(&options, "max_cached_duration", "30", 0);
//    // 设置 avformat_find_stream_info 的超时时间10毫秒
//    const char *timeOut1=[MXString(self.maxTimeOut) UTF8String];
//    av_dict_set(&options, "max_analyze_duration", timeOut1, 0);
//    av_dict_set(&options, "tune", "zerolatency", 0);
////  无缓存，解码时有效
//    av_dict_set(&options, "fflags", "nobuffer", 0);
//    av_dict_set(&options, "buffer_size", "1024000", 0);
//    av_dict_set(&options, "fflags", "nobuffer", 0);
//    av_dict_set(&options, "max_delay", "1000000", 0);//设置最大时延
    
    return options;
}


-(void)rtsp_open:(NSString *)url{
    self.urlPath = url;
    
    //=========================== 创建AVFormatContext结构体 ===============================//
    self->format_ctx = avformat_alloc_context();//该函数用于分配空间创建一个AVFormatContext对象，并且强调使用avformat_free_context方法来清理并释放该对象的空间。
    self->pAVCodecContext_video = NULL;
    AVCodec *pAVCodec_video = NULL;//AVCodec是存储编解码器信息的结构体
    AVCodecParameters *pAVCodePar_video = avcodec_parameters_alloc(); //从AVCodecParameters的定义来看，很多参数都是编码的时候用到的，解码的时候并没有用到，毕竟FFmpeg不仅负责解码，也不负责编码。
    AVPacket *pAVPacket = av_packet_alloc(); ;                                  // 是存储压缩编码数据相关信息的结构体 ffmpeg单帧数据包
    self->pAVFrame_video= av_frame_alloc();                                 // AVFrame  ffmpeg单帧缓存
    self->pAVFrameRGB32_video = av_frame_alloc();                            // ffmpeg单帧缓存转换颜色空间后的缓存
    self->pAVCodeParseContext_video= NULL;
    self->pSwsContext_video= NULL;                             // ffmpeg编码数据格式转换

    
    av_register_all();//注册封装器与解封装器
    avformat_network_init();//网络初始化

    int ret = -1;
    int numBytes = 0;                                                           // 解码后的数据长度
    self->outBuffer = NULL;                                                // 解码后的数据存放缓存区
 // open rtsp: Open an input stream and read the header. The codecs are not opened
    //const char* url = "rtsp://admin:genepoint2020@192.168.100.14:554/cam/realmonitor?channel=1&subtype=0";
    // audio/video stream index
    int video_stream_index = -1;
    AVInputFormat *ifmt =NULL;
    AVDictionary* options=[self setFFMOptions];
    char * aurl = (char*)[self.urlPath UTF8String];
    
    //==================================== 打开文件 ======================================//
    
    /**
     int avformat_open_input(AVFormatContext **ps, const char *url, AVInputFormat *fmt, AVDictionary **options);
     
     * @param ps 指向用户提供的AVFormatContext的指针(由avformat_alloc_context分配)。 函数调用成功之后处理过的AVFormatContext结构体。 *注意用户提供的AVFormatContext将在失败时被释放。
     * @param url 要打开的流的url。
     * @param fmt 如果非null，强制指定AVFormatContext中AVInputFormat的。这个参数一般情况下可以设置为NULL，这样FFmpeg可以自动检测AVInputFormat。
     * @param options 一个包含AVFormatContext和demuxer-private选项的字典。附加的一些选项，一般情况下可以设置为NULL。
     */
    ret =avformat_open_input(&self->format_ctx, aurl, ifmt, options?&options:NULL);//该函数用于打开多媒体数据并且获得一些相关的信息。
        if (ret != 0) {
            fprintf(stderr, "fail to open url: %s, return value: %d", [url UTF8String], ret);
            return;
        }
        // Read packets of a media file to get stream information
    
    
    //=================================== 获取视频流信息 ===================================//
    
    /**
     int avformat_find_stream_info(AVFormatContext *ic, AVDictionary **options);
     
     * @param ic 输入的AVFormatContext。
     * @param ic options：额外的选项
     该函数主要用于给每个媒体流（音频/视频）的AVStream结构体赋值。我们大致浏览一下这个函数的代码，会发现它其实已经实现了解码器的查找，解码器的打开，视音频帧的读取，视音频帧的解码等工作。换句话说，该函数实际上已经“走通”的解码的整个流程
     */
    
        ret = avformat_find_stream_info(self->format_ctx, NULL);//该函数可以读取一部分视音频数据并且获得一些相关的信息。
        if (ret < 0) {
            fprintf(stderr, "fail to get stream information: %d", ret);
            return;
        }
        fprintf(stdout, "Number of elements in AVFormatContext.streams: %d", self->format_ctx->nb_streams);//AVFormatContext.streams中的元素数量
        for (int i = 0; i < self->format_ctx->nb_streams; ++i) {//unsigned int nb_streams：视音频流的个数
            
            const AVStream *stream = self->format_ctx->streams[i];//AVStream **streams：视音频流
            
            fprintf(stdout, "type of the encoded data: %d", stream->codecpar->codec_id);
            
            /*
             AVCodecParameters *codecpar;
             
            AVCodecParameters与AVCodecContext里的参数有很多相同的，但是没有函数。看来伟大的极客们，是想把一些编解码器的参数从AVCodecContext分离出来，因为AVCodecContext这个结构体实在是太大了。
                  @ enum AVMediaType codec_type：编解码器的类型（视频，音频...）
                  @ enum AVCodecID codec_id：标示特定的编解码器
                  @ int format：像素格式/采样数据格式
                  @ int bit_rate：平均比特率
                  @ uint8_t *extradata; int extradata_size：针对特定编码器包含的附加信息（例如对于H.264解码器来说，存储SPS，PPS等）
                  @ int width, height：如果是视频的话，代表宽和高
                  @ int refs：运动估计参考帧的个数（H.264的话会有多帧，MPEG2这类的一般就没有了）
                  @ int sample_rate：采样率（音频）
                  @ uint64_t channel_layout：声道格式
                  @ int channels：声道数（音频）
                  @ int profile：型（H.264里面就有，其他编码标准应该也有）
                  @ int level：级（和profile差不太多）
                   从AVCodecParameters的定义来看，很多参数都是编码的时候用到的，解码的时候并没有用到，毕竟FFmpeg不仅负责解码，也负责编码。
          */
            
            if (stream->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) { //@ enum AVMediaType codec_type：编解码器的类型（视频，音频...）
                
                video_stream_index = i;
                // AVCodecParameters 对找到的视频流寻解码器
                pAVCodePar_video = stream->codecpar;
                
                //=================================  查找解码器 ===================================//
                /**
                 AVCodec *avcodec_find_decoder(enum AVCodecID id);//用于查找FFmpeg的解码器。
                 * @param id 输入的AVFormatContext。 函数的参数是一个编码器的ID，返回查找到的编码器（没有找到就返回NULL）。
                 */
                
                pAVCodec_video = avcodec_find_decoder(stream->codecpar->codec_id);//用于查找FFmpeg的解码器。
                if (!pAVCodec_video) {
                    video_stream_index = -1;
                    break;
                }
                
                /**
                 AVCodecParserContext *av_parser_init(int codec_id);//初始化AVCodecParserContext。其参数是codec_id,所以同时只能解析一种AVCodecParser用于解析输入的数据流并把它们分成一帧一帧的压缩编码数据
                 。比较形象的说法就是把长长的一段连续的数据“切割”成一段段的数据。
                 */
                self->pAVCodeParseContext_video = av_parser_init(pAVCodec_video->id);
                if (! self->pAVCodeParseContext_video) {
                    video_stream_index = -1;
                    break;
                }
                
                /**
                 AVCodecContext *avcodec_alloc_context3(const AVCodec *codec);//初始化解码上下文
                 申请AVCodecContext空间。需要传递一个编码器，也可以不传，但不会包含编码器。
                 @param codec 参数codec非空，这利用codec的指定参数初始化AVCodecContext，如果codec为空，则不初始化ctx的特定编码器参数ctx->codec，这个参数很重要
                 */
                self->pAVCodecContext_video = avcodec_alloc_context3(pAVCodec_video);//初始化解码上下文
                if (!self->pAVCodecContext_video) {
                }
                AVDictionary *codec_opts = NULL;
                av_dict_set(&codec_opts, "threads", "auto", 0);
                av_dict_set(&codec_opts, "reorder_queque_size", "350000", 0);
                av_dict_set(&codec_opts, "max_delay", "1000000", 0);
                //设置udp的接收缓冲 视频缓冲区大小
                av_dict_set(&codec_opts, "buffer_size", "8388608", 0);
                av_dict_set(&codec_opts, "framedrop", "1", 0);
                //探测长度，可以通过播放器设置，默认探测长度5000000字节
                av_dict_set(&codec_opts, "probesize", "1000000", 0);
                av_dict_set(&codec_opts, "max_cached_duration", "30", 0);
                //无缓存，解码时有效
                av_dict_set(&codec_opts, "fflags", "nobuffer", 0);
                av_dict_set_int(&codec_opts, "video_track_timescale", 25, 0);
                av_dict_set(&codec_opts, "preset", "superfast", 0);
                av_dict_set(&codec_opts, "tune", "zerolatency", 0);
                
                //================================  打开解码器 ===================================//
                
                /**
                 int avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options);
                 // 该函数用于初始化一个视音频编解码器的AVCodecContext。
                 
                 @param avctx 需要初始化的AVCodecContext。
                 @param codec 输入的AVCodec
                 @param options 一些选项。例如使用libx264编码的时候，“preset”，“tune”等都可以通过该参数设置。
                 
                 简单梳理一下avcodec_open2()所做的工作
                 （1）为各种结构体分配内存（通过各种av_malloc()实现）。
                 （2）将输入的AVDictionary形式的选项设置到AVCodecContext。
                 （3）其他一些零零碎碎的检查，比如说检查编解码器是否处于“实验”阶段。
                 （4）如果是编码器，检查输入参数是否符合编码器的要求
                 （5）调用AVCodec的init()初始化具体的解码器。
                 
                 */
                
                
                if (avcodec_open2(self->pAVCodecContext_video, pAVCodec_video, &codec_opts) < 0) {
                    video_stream_index = -1;
                    break;
                }
                fprintf(stdout, "dimensions of the video frame in pixels:  %d, height: %d, pixel format: %d",
                        stream->codecpar->width, stream->codecpar->height, stream->codecpar->format);//视频帧的像素尺寸:%d，高度:%d，像素格式
            }
        }
    
    
        if (video_stream_index == -1) {
            fprintf(stderr, "no video stream");
            return;
        }
        // 对拿到的原始数据格式进行缩放转换为指定的格式高宽大小
    
    
    //================================ 设置数据转换参数 ================================//
    /**
     struct SwsContext *sws_getContext(int srcW, int srcH, enum AVPixelFormat srcFormat,
                                       int dstW, int dstH, enum AVPixelFormat dstFormat,
                                       int flags, SwsFilter *srcFilter,
                                       SwsFilter *dstFilter, const double *param);
     //sws_getContext()：初始化一个SwsContext。
     
     @param srcW  源图像的宽
     @param srcH 源图像的高
     @param srcFormat 源图像的像素格式
     @param dstW 目标图像的宽
     @param dstH 目标图像的高
     @param dstFormat 目标图像的像素格式
     @param flags 设定图像拉伸使用的算法
     
     成功执行的话返回生成的SwsContext，否则返回NULL。
     */
    
    self->pSwsContext_video = sws_getContext(
                pAVCodePar_video->width,
                pAVCodePar_video->height,
                AV_PIX_FMT_YUV420P,
                pAVCodePar_video->width,
                pAVCodePar_video->height,
                AV_PIX_FMT_RGBA,
                SWS_FAST_BILINEAR,
                                           NULL,
                                           NULL,
                                           NULL
        );
    
    //==================================== 分配空间 ==================================//
    /**
     
     int av_image_get_buffer_size(enum AVPixelFormat pix_fmt, int width, int height, int align);
     函数的作用是通过指定像素格式、图像宽、图像高来计算所需的内存大小。
     
     @param pix_fmt  要申请内存的图像的像素格式。
     @param width  要申请内存的图像宽度
     @param height  要申请内存的图像高度。
     @param align  用于内存对齐的值。
     返回值：所申请的内存空间的总大小。如果是负值，表示申请失败
     */
    
        numBytes = av_image_get_buffer_size(
                AV_PIX_FMT_RGBA,
                pAVCodePar_video->width,
                pAVCodePar_video->height,
                1
        );
    
    
    /**
     void *av_malloc(size_t size) av_malloc_attrib av_alloc_size(1);
     av_malloc()是FFmpeg中最常见的内存分配函数
     
     */
    
    //=========================== 分配AVPacket结构体 ===============================//
    self->outBuffer = (u_char *) av_malloc(numBytes);
        // pAVFrame32的data指针指向了outBuffer
    
    //会将pFrameRGB的数据按RGB格式自动"关联"到buffer  即pFrameRGB中的数据改变了
    //out_buffer中的数据也会相应的改变
        av_image_fill_arrays(
                             self->pAVFrameRGB32_video->data,
                             self->pAVFrameRGB32_video->linesize,
                             self->outBuffer,
                AV_PIX_FMT_RGBA,
                pAVCodePar_video->width,
                pAVCodePar_video->height,
                1
        );
    
    /**
     int av_read_frame(AVFormatContext *s, AVPacket *pkt);//是读取码流中的音频若干帧或者视频一帧
     解码视频的时候，每解码一个视频帧，需要先调用 av_read_frame()获得一帧视频的压缩数据，然后才能对该数据进行解码
     
     @param s 输入的AVFormatContext
     @param pkt 输出的AVPacket
     
     
     */
    

    //===========================  读取视频信息 ===============================//
        while (1) {
            ret = av_read_frame(self->format_ctx, pAVPacket);//读取的是一帧视频  数据存入一个AVPacket的结构中
           
            if (ret == AVERROR(EAGAIN)) {
                av_usleep(10000);
                continue;
            }
            
            if (ret < 0) {
                fprintf(stderr, "error or end of file: %d", ret);
                continue;
            }
            
            /**
             int avcodec_send_packet(AVCodecContext *avctx, const AVPacket *avpkt);
             这个函数是将要解码的数据包送入解码器
             
             在FFMPEG中 avcodec_send_packet() 和 avcodec_receive_frame() 通常是同时使用的，先调用 avcodec_send_packet() 送入要解码的数据包，然后调用 avcodec_receive_frame()获取解码后的音视频数据。但是需要注意的是：解码器内部是有缓冲区数据处理的，因此并不保证每送入一个数据包，就一定有相应的解码音视频帧输出，这两个函数对于数据处理，在时序上并不同步
             
             通常解码开始，通过avcodec_send_packet()送入几十个数据包，对应的avcodec_receive_frame()都没有音视频帧输出。等送入的数据包足够多后，avcodec_receive_frame()才开始输出前面一开始送入进行解码的音视频帧。最后几十没有数据包送入了，也要调用avcodec_send_packet()送入空数据包，以驱动解码模块继续解码缓冲区中的数据，此时avcodec_receive_frame()还是会有音视频帧输出，直到返回AVERROR_EOF才表示所有数据包解码完成。
             
             @param avptk 其中 avptk参数可以为空(NULL)，当avpkt参数为空时表示继续刷新内部缓冲区的解码数据，此时还可能会有解码后的音视频帧输出。
             */
            
            if (pAVPacket->stream_index == video_stream_index) {
                fprintf(stdout, "video stream, packet size: %d", pAVPacket->size);
                ret = avcodec_send_packet(self->pAVCodecContext_video,pAVPacket);// 送入要解码的数据包
                if( 0 != ret){
                    continue;
            }
                
                
                
            /**
             int avcodec_receive_frame(AVCodecContext *avctx, AVFrame *frame);
             实现视频帧的解码 从解码器返回解码后的输出数据。
             
             @param avctx 第一个参数视频解码的上下文，与上面接口一致
             @param avctx 解码后的视频帧数据
             */
                
                
                
            while (avcodec_receive_frame(self->pAVCodecContext_video,self->pAVFrame_video) == 0){
                
                CGFloat width= pAVCodePar_video->width;
                CGFloat height=pAVCodePar_video->height;
                
                /**
                 
                 int sws_scale(struct SwsContext *c, const uint8_t *const srcSlice[],
                               const int srcStride[], int srcSliceY, int srcSliceH,
                               uint8_t *const dst[], const int dstStride[]);
                 
                 函数主要是用来做视频像素格式和分辨率的转换，其优势在于：可以在同一个函数里实现：1.图像色彩空间转换， 2:分辨率缩放，3:前后图像滤波处理。不足之处在于：效率相对较低，不如libyuv或shader
                 
                 @param c 转换格式的上下文
                 @param srcSlice[] 输入图像的每个颜色通道的数据指针。其实就是解码后的AVFrame中的data[]数组。因为不同像素的存储格式不同，所以srcSlice[]维数
                 也有可能不同。
                 
                 @param srcStride[] 输入图像的每个颜色通道的跨度。.也就是每个通道的行字节数，对应的是解码后的AVFrame中的linesize[]数组。
                 
                 @param srcSliceY,srcSliceH 参数int srcSliceY, int srcSliceH,定义在输入图像上处理区域，srcSliceY是起始位置，srcSliceH是处理多少行。。如果srcSliceY=0，srcSliceH=height，表示一次性处理完整个图像。这种设置是为了多线程并行，例如可以创建两个线程，第一个线程处理 [0, h/2-1]行，第二个线程处理 [h/2, h-1]行。并行处理加快速度。
                 
                 @param dst[],dstStride[] 定义输出图像信息（输出的每个颜色通道数据指针，每个颜色通道行字节数）
                 */
                
                
                sws_scale(self->pSwsContext_video,
                          (const uint8_t * const *)self->pAVFrame_video->data,
                          self->pAVFrame_video->linesize,
                          0,
                          pAVCodePar_video->height,
                          self->pAVFrameRGB32_video->data,
                          self->pAVFrameRGB32_video->linesize);
                
                CGBitmapInfo bitmapInfo = kCGBitmapByteOrderDefault;
                CFDataRef data = CFDataCreateWithBytesNoCopy(kCFAllocatorDefault, self->pAVFrameRGB32_video->data[0],  self->pAVFrameRGB32_video->linesize[0]*height,kCFAllocatorNull);
                CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
                CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
                CGImageRef cgImage = CGImageCreate(width,
                                                   height,
                                                   8,
                                                   24,
                                                   self->pAVFrameRGB32_video->linesize[0],
                                                   colorSpace,
                                                   bitmapInfo,
                                                   provider,
                                                   NULL,
                                                   NO,
                                                   kCGRenderingIntentDefault);
                CGColorSpaceRelease(colorSpace);
                UIImage *image = [UIImage imageWithCGImage:cgImage];
                if ([self.delegate respondsToSelector:@selector(everyFrameRGBImage:)]) {
                    [self.delegate everyFrameRGBImage:image];
                }
                CGImageRelease(cgImage);
                CGDataProviderRelease(provider);
            }
        }
        av_packet_unref(pAVPacket);
    }
    
    //===========================释放所有指针===============================//
    av_parser_close(self->pAVCodeParseContext_video);
    av_frame_free(&self->pAVFrame_video);
    av_frame_free(&self->pAVFrameRGB32_video);
    av_free(self->outBuffer);
    av_free(self->pSwsContext_video);
    avcodec_free_context(&self->pAVCodecContext_video);
    avformat_close_input(&self->format_ctx);
}

-(void)dealloc{
    av_parser_close(self->pAVCodeParseContext_video);
    av_frame_free(&self->pAVFrame_video);
    av_frame_free(&self->pAVFrameRGB32_video);
    av_free(self->outBuffer);
    av_free(self->pSwsContext_video);
    avcodec_free_context(&self->pAVCodecContext_video);
    avformat_close_input(&self->format_ctx);
}
@end



```

```
//
//  YQMediaPlayer.h
//  YQFFmpeg
//
//  Created by iWolf on 2022/8/4.
//

#import <Foundation/Foundation.h>
#import <UIKit/UIKit.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

NS_ASSUME_NONNULL_BEGIN

@protocol YQMediaPlayerDelegate <NSObject>

/////frame image
-(void)everyFrameRGBImage:(UIImage *_Nullable)image;

@end

@interface YQMediaPlayer : NSObject

@property (nonatomic,weak)id <YQMediaPlayerDelegate>delegate;
-(void)rtsp_open:(NSString *)url;

@end

NS_ASSUME_NONNULL_END


```
<br>
转载请注明：[iWolf的博客](https://iwolfsex.github.io/) » [FFMpeg拉流过程](http://iWolf.com/2023/02/FFMpeg拉流过程/)  
