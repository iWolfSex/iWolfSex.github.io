---
layout: post
title: 局网内点对点语音对讲二
date: 2018-03-08
tag: iOS
---

在[局网内点对点语音对讲一](http://iWolf.com/2018/03/局网内点对点语音对讲一/)中我讲解了利用socket接口的接口调TCP传输协议实现了音频数据在网络中的传输。这一讲主要讲解音频数据的采集、压缩和解压。我把音频的采集和播放封装成了两个类。
音频的录取我们要用的到iOS提供的AudioToolbox框架。这个库是C的接口，用于在线流媒体音频的播放。这里主要讲解Audio Queue对音频进行播放。利用Audio Queue可是实现一边采集音频数据一边传输音频数据。类似于打电话吧。
什么是Audio Queue？我在网上找了一个截图。是人家从Apple的官方文档上截下以下该图。
Apple的官方文档上截下以下该图

<img src="/images/posts/局网内点对点语音对讲二/局网内点对点语音对讲二.png" > 

理解AudioQueue的工作原理。我也是摘录其他人的原话如下。
1. 用户调用相应的方法，将音频数据从硬盘中读入到AudioQueue的缓冲区中，并将缓冲区送入音频队列。 
2. 用户App通过AudioQueue提供的接口，告诉外放设备，缓冲区中已经有数据，可以拿去播放。 
3. 当一个缓冲区中的音频数据播放完毕之后，AudioQueue告诉用户，当前有一个空的缓冲区可以用来给你填充数据。 
4. 重复以上步骤，直至数据播放完毕。

###  利用AudioToolbox实现音频流数据的采集和播放的源码

采集类的封装

头文件

```
//
//  RMCQRecord.h
//  RMCQAudioQueue
//
//  Created by 刘超 on 14-4-2.
//  Copyright (c) 2014年 刘超. All rights reserved.
//

#import <Foundation/Foundation.h>
#import <AudioToolbox/AudioToolbox.h>
#import <AudioToolbox/AudioFile.h>
#define kNumberBuffers 3

#define t_sample SInt16

#define kSamplingRate 20000
#define kNumberChannels 1
#define kBitsPerChannels (sizeof(t_sample) * 8)
#define kBytesPerFrame (kNumberChannels * sizeof(t_sample))
//#define kFrameSize (kSamplingRate * sizeof(t_sample))
#define kFrameSize 1000

typedef struct AQCallbackStruct
{
    AudioStreamBasicDescription mDataFormat;
    AudioQueueRef queue;
    AudioQueueBufferRef mBuffers[kNumberBuffers];
    AudioFileID outputFile;
    
    unsigned long frameSize;
    long long recPtr;
    int run;
} AQCallbackStruct;

@interface RMCQRecord : NSObject{
    AQCallbackStruct aqc;
    AudioFileTypeID fileFormat;
    long audioDataLength;
    Byte audioByte[999999];
    long audioDataIndex;
}
- (id) init;
- (void) start;
- (void) stop;
-(void) reset;
-(void) dispose;
- (void) pause;
-(void)close;
- (Byte*) getBytes;
- (void) processAudioBuffer:(AudioQueueBufferRef) buffer withQueue:(AudioQueueRef) queue;

@property (nonatomic, assign) AQCallbackStruct aqc;
@property (nonatomic, assign) long audioDataLength;
@end

```

源文件

```
//
//  RMCQRecord.m
//  RMCQAudioQueue
//
//  Created by 刘超 on 14-4-2.
//  Copyright (c) 2014年 刘超. All rights reserved.
//

#import "RMCQRecord.h"

@implementation RMCQRecord
@synthesize aqc;
@synthesize audioDataLength;


static void AQInputCallback (void * inUserData,
                             AudioQueueRef inAudioQueue,
                             AudioQueueBufferRef inBuffer,
                             const AudioTimeStamp * inStartTime,
                             unsigned long inNumPackets,
                             const AudioStreamPacketDescription * inPacketDesc)
{
    
    RMCQRecord * engine = (__bridge RMCQRecord *) inUserData;
    if (inNumPackets > 0)
    {
        [engine processAudioBuffer:inBuffer withQueue:inAudioQueue];
    }
    
    if (engine.aqc.run)
    {
        AudioQueueEnqueueBuffer(engine.aqc.queue, inBuffer, 0, NULL);
    }
}

- (id) init
{
    self = [super init];
    
    if (self)
    {
        
        aqc.mDataFormat.mSampleRate = kSamplingRate;
        aqc.mDataFormat.mFormatID = kAudioFormatLinearPCM;
        aqc.mDataFormat.mFormatFlags = kLinearPCMFormatFlagIsSignedInteger |kLinearPCMFormatFlagIsPacked;
        aqc.mDataFormat.mFramesPerPacket = 1;
        aqc.mDataFormat.mChannelsPerFrame = kNumberChannels;
        
        aqc.mDataFormat.mBitsPerChannel = kBitsPerChannels;
        
        aqc.mDataFormat.mBytesPerPacket = kBytesPerFrame;
        aqc.mDataFormat.mBytesPerFrame = kBytesPerFrame;
        
        aqc.frameSize = kFrameSize;
        
        AudioQueueNewInput(&aqc.mDataFormat, AQInputCallback, (__bridge void *)(self), NULL, kCFRunLoopCommonModes,0, &aqc.queue);
        
        for (int i=0;i<kNumberBuffers;i++)
        {
            AudioQueueAllocateBuffer(aqc.queue, aqc.frameSize, &aqc.mBuffers[i]);
            AudioQueueEnqueueBuffer(aqc.queue, aqc.mBuffers[i], 0, NULL);
        }
        aqc.recPtr = 0;
        aqc.run = 1;
    }
    audioDataIndex = 0;
    return self;
}

- (void) dealloc
{
    AudioQueueStop(aqc.queue, true);
    aqc.run = 0;
    AudioQueueDispose(aqc.queue, true);
}

- (void) start
{
    AudioQueueStart(aqc.queue, NULL);
}

- (void) stop
{
    AudioQueueStop(aqc.queue, true);
}

- (void) pause
{
    AudioQueuePause(aqc.queue);
}

-(void) reset{
    AudioQueueReset(aqc.queue);
}
-(void)dispose{
    AudioQueueDispose(aqc.queue,true);
}
-(void)close{
    AudioFileClose(aqc.outputFile);
}
- (Byte *)getBytes
{
    return audioByte;
}

- (void) processAudioBuffer:(AudioQueueBufferRef) buffer withQueue:(AudioQueueRef) queue
{
    NSLog(@"processAudioData :%ld", buffer->mAudioDataByteSize);
    //处理data：忘记oc怎么copy内存了，于是采用的C++代码，记得把类后缀改为.mm。同Play
    memcpy(audioByte+audioDataIndex, buffer->mAudioData, buffer->mAudioDataByteSize);
    audioDataIndex +=buffer->mAudioDataByteSize;
    audioDataLength = audioDataIndex;
}

@end

```
播放类的封装

头文件

```
//
//  RMCQPlay.h
//  RMCQAudioQueue
//
//  Created by 刘超 on 14-4-2.
//  Copyright (c) 2014年 刘超. All rights reserved.
//

#import <Foundation/Foundation.h>
#import <AudioToolbox/AudioToolbox.h>

// Audio Settings
#define kNumberBuffers      3
#define t_sample             SInt16
#define kSamplingRate       20000
#define kNumberChannels     1
#define kBitsPerChannels    (sizeof(t_sample) * 8)
#define kBytesPerFrame      (kNumberChannels * sizeof(t_sample))
//#define kFrameSize          (kSamplingRate * sizeof(t_sample))
#define kFrameSize          1000


#define QUEUE_BUFFER_SIZE  2//队列缓冲个数
#define EVERY_READ_LENGTH  10240 //每次从文件读取的长度
#define MIN_SIZE_PER_FRAME 10240 //每侦最小数据长度

@interface RMCQPlay : NSObject
{
    //音频参数
    AudioStreamBasicDescription audioDescription;
    // 音频播放队列
    AudioQueueRef audioQueue;
    // 音频缓存
    AudioQueueBufferRef audioQueueBuffers[QUEUE_BUFFER_SIZE];
}

-(void)Play:(Byte *)audioByte Length:(long)len;
-(void)Stop;

@end

```

源文件

```
//
//  RMCQPlay.m
//  RMCQAudioQueue
//
//  Created by 刘超 on 14-4-2.
//  Copyright (c) 2014年 刘超. All rights reserved.
//

#import "RMCQPlay.h"
@interface RMCQPlay()
{
    Byte *audioByte;
    long audioDataIndex;
    long audioDataLength;
}
@end
@implementation RMCQPlay

//回调函数(Callback)的实现
static void BufferCallback(void *inUserData,AudioQueueRef inAQ,AudioQueueBufferRef buffer){
    
    NSLog(@"processAudioData :%u", (unsigned int)buffer->mAudioDataByteSize);
    
    RMCQPlay* player=(__bridge RMCQPlay*)inUserData;
    
    [player FillBuffer:inAQ queueBuffer:buffer];
}

void soundCompleteCallBack(SystemSoundID soundID, void * clientDate) {
    NSLog(@"播放完成");
    AudioServicesDisposeSystemSoundID(soundID);
}

//缓存数据读取方法的实现
-(void)FillBuffer:(AudioQueueRef)queue queueBuffer:(AudioQueueBufferRef)buffer
{
    if(audioDataIndex + EVERY_READ_LENGTH < audioDataLength)
    {
        memcpy(buffer->mAudioData, audioByte+audioDataIndex, EVERY_READ_LENGTH);
        audioDataIndex += EVERY_READ_LENGTH;
        buffer->mAudioDataByteSize =EVERY_READ_LENGTH;
        AudioQueueEnqueueBuffer(queue, buffer, 0, NULL);
    }
    
}

-(void)SetAudioFormat
{
    ///设置音频参数
    audioDescription.mSampleRate  = kSamplingRate;//采样率
    audioDescription.mFormatID    = kAudioFormatLinearPCM;
    audioDescription.mFormatFlags =  kAudioFormatFlagIsSignedInteger;//|kAudioFormatFlagIsNonInterleaved;
    audioDescription.mChannelsPerFrame = kNumberChannels;
    audioDescription.mFramesPerPacket  = 1;//每一个packet一侦数据
    audioDescription.mBitsPerChannel   = kBitsPerChannels;//av_get_bytes_per_sample(AV_SAMPLE_FMT_S16)*8;//每个采样点16bit量化
    audioDescription.mBytesPerFrame    = kBytesPerFrame;
    audioDescription.mBytesPerPacket   = kBytesPerFrame;
    
    [self CreateAudioQueue];
}

-(void)CreateAudioQueue
{
    [self Cleanup];
    //使用player的内部线程播
    AudioQueueNewOutput(&audioDescription, BufferCallback, (__bridge void *)(self), nil, nil, 0, &audioQueue);
    if(audioQueue)
    {
        ////添加buffer区
        for(int i=0;i<QUEUE_BUFFER_SIZE;i++)
        {
            int result =  AudioQueueAllocateBuffer(audioQueue, EVERY_READ_LENGTH, &audioQueueBuffers[i]);
            ///创建buffer区，MIN_SIZE_PER_FRAME为每一侦所需要的最小的大小，该大小应该比每次往buffer里写的最大的一次还大
            NSLog(@"AudioQueueAllocateBuffer i = %d,result = %d",i,result);
        }
    }
}

-(void)Cleanup
{
    if(audioQueue)
    {
        NSLog(@"Release AudioQueueNewOutput");
        
        [self Stop];
        for(int i=0; i < QUEUE_BUFFER_SIZE; i++)
        {
            AudioQueueFreeBuffer(audioQueue, audioQueueBuffers[i]);
            audioQueueBuffers[i] = nil;
        }
        audioQueue = nil;
    }
}

-(void)Stop
{
    NSLog(@"Audio Player Stop");
    
    AudioQueueFlush(audioQueue);
    AudioQueueReset(audioQueue);
    AudioQueueStop(audioQueue,TRUE);

}

-(void)Play:(Byte *)byte Length:(long)len
{
    [self Stop];
    audioByte = byte;
    audioDataLength = len;
    
    NSLog(@"Audio Play Start >>>>>");
    
    [self SetAudioFormat];
    
    AudioQueueReset(audioQueue);
    audioDataIndex = 0;
    for(int i=0; i<QUEUE_BUFFER_SIZE; i++)
    {
        [self FillBuffer:audioQueue queueBuffer:audioQueueBuffers[i]];
    }
    AudioQueueStart(audioQueue, NULL);
}
@end

```

###  PCM数据转AMR。即音频数据的编码于解码。

PCM:在我看来就是原始的音频数据。未经过压缩处理。
AMR，全称是：Adaptive Multi-Rate，自适应多速率，是一种音频编码文件格式，专用于有效地压缩语音频率。
AMR音频主要用于移动设备的音频压缩，压缩比非常高，但是音质比较差，主要用于语音类的音频压缩，不适合对音质要求较高的音乐类音频的压缩

这部分代码是在网上找的。我贴出来供参考。

头文件


```
//
//  RecordAmrCode.h
//  VoiceChat
//
//  Created by MacOS on 14-9-15.
//  Copyright (c) 2014年 MacOS. All rights reserved.
//

/**
 *  使用audioqueque来实时录音，边录音边转码，可以设置自己的转码方式。从PCM数据转
 */

#import <Foundation/Foundation.h>
@interface RecordAmrCode : NSObject

//将PCM格式Data进行编码，转换为AMR格式
- (NSData *)encodePCMDataToAMRData:(NSData *)pcmData;

//讲AMR格式Data解码，转换为PCM格式
- (NSData *)decodeAMRDataToPCMData:(NSData *)amrData;

typedef struct _RTP_header
{
    /* byte 0 */
#if (BYTE_ORDER == LITTLE_ENDIAN)
    unsigned char csrc_len:4;   /* expect 0 */
    unsigned char extension:1;  /* expect 1, see RTP_OP below */
    unsigned char padding:1;    /* expect 0 */
    unsigned char version:2;    /* expect 2 */
#elif (BYTE_ORDER == BIG_ENDIAN)
    unsigned char version:2;    /* 版本号 */
    unsigned char padding:1;    /* 填充 */
    unsigned char extension:1;  /* 填充头 */
    unsigned char csrc_len:4;   /* 作用源个数 */
#else
#error Neither big nor little
#endif
    /* byte 1 */
#if (BYTE_ORDER == LITTLE_ENDIAN)
    unsigned char payload:7;    /*  */
    unsigned char marker:1;     /*except 1  */
#elif (BYTE_ORDER == BIG_ENDIAN)
    unsigned char marker:1;     /* 帧边界标识 */
    unsigned char payload:7;    /* 负载类型 */
#endif
    /* bytes 2, 3 */
    unsigned short seq_no;      /* 序列号*/
    /* bytes 4-7 */
    unsigned int timestamp;     /* 时间 */
    /* bytes 8-11 */
    unsigned int ssrc;          /* stream number is used here. */
} RTP_header;

@end


```

源文件


```
//
//  RecordAmrCode.m
//  VoiceChat
//
//  Created by MacOS on 14-9-15.
//  Copyright (c) 2014年 MacOS. All rights reserved.
//

#import "RecordAmrCode.h"

#define PCM_FRAME_SIZE 160 // 8khz 8000*0.02=160
#define MAX_AMR_FRAME_SIZE 32
#define AMR_FRAME_COUNT_PER_SECOND 50

#define AMR_MAGIC_NUMBER "#!AMR\n"

//amr编码、解码
#import "interf_enc.h"
#include "interf_dec.h"

@implementation RecordAmrCode

- (NSData *)encodePCMDataToAMRData:(NSData *)pcmData
{
    void *destate = 0;
    RTP_header rtpHead;
    memset(&rtpHead,0,sizeof(RTP_header));
    rtpHead.marker = 1;
    rtpHead.payload = 0;
    int nLen = 0;
    int nSLen = 0;
    
    // amr 压缩句柄
    destate = Encoder_Interface_init(0);
    if (destate == 0) {
        return nil;
    }
    
    NSMutableData *amrData = [NSMutableData data];
    
    //编码
    const void *recordingData = pcmData.bytes;
    NSUInteger pcmLen = pcmData.length;
    
    if (pcmLen<=0){
        return nil;
    }
    if (pcmLen%2!=0){
        pcmLen--; //防止意外，如果不是偶数，情愿减去最后一个字节。
        NSLog(@"不是偶数");
    }
    
    unsigned char buffer[320];
    for (int i =0; i < pcmLen ;i+=160*2) {
        short *pPacket = (short *)((unsigned char*)recordingData+i);
//        if (pcmLen-i<160*2){
//            continue; //不是一个完整的就拜拜
//        }
        
        memset(buffer, 0, sizeof(buffer));
        //encode
        int recvLen = Encoder_Interface_Encode(destate,MR475,pPacket,buffer,0);
       
        //tcp传输方式所以加tcp头
//        nLen = recvLen + sizeof(RTP_header);
        nLen = recvLen;
        unsigned char amrBuf[336];
        memset(amrBuf, 0, sizeof(amrBuf));
        amrBuf[0] = '$';
        amrBuf[1] = 2;
        amrBuf[2] = nLen>>8;
        amrBuf[3] = nLen&0xff;
        memcpy(amrBuf+4,&rtpHead,sizeof(RTP_header));
        memcpy(amrBuf+4+sizeof(RTP_header),buffer,recvLen);
        
        nSLen = sizeof(RTP_header) + 4 + recvLen;

        if (recvLen>0) {
            NSData *data = [NSData dataWithBytes:amrBuf length:nSLen];
            [amrData appendData:data];
        }
    }

    return amrData;
}

- (NSData *)decodeAMRDataToPCMData:(NSData *)amrData
{
    void *destate;
    int nFrameCount = 0;
    int stdFrameSize;
    int nTemp;
    char bErr = 0;
    unsigned char stdFrameHeader;
    
    unsigned char amrFrame[MAX_AMR_FRAME_SIZE];
    short pcmFrame[PCM_FRAME_SIZE];
    
    if (amrData.length <= 0) {
        return nil;
    }
    
    const char* rfile = [amrData bytes];
    int maxLen = [amrData length];
    int pos = 0;
   
    NSMutableData* pcmData = [[NSMutableData alloc]init];
    
    /* init decoder */
    destate = Decoder_Interface_init();
    
    // 读第一帧 - 作为参考帧
    memset(amrFrame, 0, sizeof(amrFrame));
    memset(pcmFrame, 0, sizeof(pcmFrame));
    
    //参数一次是接收到的amr数据,下次开始点,一个amrFrame,帧大小 帧头
    nTemp = ReadAMRFrameFirstData(rfile,pos,maxLen, amrFrame, &stdFrameSize, &stdFrameHeader);
    if (nTemp==0) {
        Decoder_Interface_exit(destate);
        return nil;
    }
    pos += nTemp;
    // 解码一个AMR音频帧成PCM数据
    Decoder_Interface_Decode(destate, amrFrame, pcmFrame, 0);
    nFrameCount++;
    //fwrite(pcmFrame, sizeof(short), PCM_FRAME_SIZE, fpwave);
    [pcmData appendBytes:pcmFrame length:PCM_FRAME_SIZE*sizeof(short)];
    
    // 逐帧解码AMR并写到pcmData里
    while(1)
    {
        memset(amrFrame, 0, sizeof(amrFrame));
        memset(pcmFrame, 0, sizeof(pcmFrame));
        //if (!ReadAMRFrame(fpamr, amrFrame, stdFrameSize, stdFrameHeader)) break;
        
        nTemp = ReadAMRFrameData(rfile,pos,maxLen, amrFrame, stdFrameSize, stdFrameHeader);
        if (!nTemp) {bErr = 1;break;}
        pos += nTemp;
        
        // 解码一个AMR音频帧成PCM数据 (8k-16b-单声道)
        Decoder_Interface_Decode(destate, amrFrame, pcmFrame, 0);
        nFrameCount++;
        //fwrite(pcmFrame, sizeof(short), PCM_FRAME_SIZE, fpwave);
        [pcmData appendBytes:pcmFrame length:PCM_FRAME_SIZE*sizeof(short)];
    }
//  NSLog(@"frame = %d", nFrameCount);
    Decoder_Interface_exit(destate);
    
    return pcmData;
}

// 读第一个帧 - (参考帧)
// 返回值: 0-出错; 1-正确
int ReadAMRFrameFirstData(char* fpamr,int pos,int maxLen, unsigned char frameBuffer[], int* stdFrameSize, unsigned char* stdFrameHeader)
{
    int nPos = 0;
    memset(frameBuffer, 0, sizeof(frameBuffer));//一帧amr数据
    //去掉rtp包的16个字节
    if(fpamr[0] == '$'){
        nPos = 16;
    }
    else{
        return 0;//不是rtp包
    }
    // 先读帧头
    stdFrameHeader[0] = fpamr[nPos];
    nPos++;
    
    if (pos+nPos >= maxLen) {
        return 0;
    }
    
    // 根据帧头计算帧大小
    *stdFrameSize = caclAMRFrameSize(*stdFrameHeader);
    
    // 读首帧
    frameBuffer[0] = *stdFrameHeader;
    if ((*stdFrameSize-1)*sizeof(unsigned char)<=0) {
        return 0;
    }
    
    memcpy(&(frameBuffer[1]), fpamr+pos+nPos, (*stdFrameSize-1)*sizeof(unsigned char));
    //fread(&(frameBuffer[1]), 1, (*stdFrameSize-1)*sizeof(unsigned char), fpamr);
    //if (feof(fpamr)) return 0;
    nPos += (*stdFrameSize-1)*sizeof(unsigned char);
    if (pos+nPos >= maxLen) {
        return 0;
    }
    
    return nPos;
}

int ReadAMRFrameData(char* fpamr,int pos,int maxLen, unsigned char frameBuffer[], int stdFrameSize, unsigned char stdFrameHeader)
{
    int nPos = 0;
    unsigned char frameHeader; // 帧头
    
    memset(frameBuffer, 0, sizeof(frameBuffer));
    
    // 读帧头
    // 如果是坏帧(不是标准帧头)，则继续读下一个字节，直到读到标准帧头
    while(1)
    {
        //去掉rtp包的16个字节
        if(fpamr[0] == '$'){
            nPos = 16;
        }
        else{
            return 0;//不是rtp包
        }

        if (pos+nPos >=maxLen) {
            return 0;
        }
        frameHeader = fpamr[pos+nPos]; pos++;
        if (frameHeader == stdFrameHeader) break;
    }
    
    // 读该帧的语音数据(帧头已经读过)
    frameBuffer[0] = frameHeader;
    //bytes = fread(&(frameBuffer[1]), 1, (stdFrameSize-1)*sizeof(unsigned char), fpamr);
    //if (feof(fpamr)) return 0;
    if ((stdFrameSize-1)*sizeof(unsigned char)<=0) {
        return 0;
    }
    memcpy(&(frameBuffer[1]), fpamr+pos+nPos, (stdFrameSize-1)*sizeof(unsigned char));
    nPos += (stdFrameSize-1)*sizeof(unsigned char);
    if (pos+nPos >= maxLen) {
        return 0;
    }
    
    return nPos;
}


// 根据帧头计算当前帧大小
int caclAMRFrameSize(unsigned char frameHeader)
{
    int mode;
    int temp1 = 0;
    int temp2 = 0;
    int frameSize;
    int amrEncodeMode[] = {4750, 5150, 5900, 6700, 7400, 7950, 10200, 12200}; // amr 编码方式
    
    temp1 = frameHeader;
    
    // 编码方式编号 = 帧头的3-6位
    temp1 &= 0x78; // 0111-1000
    temp1 >>= 3;
    
    mode = amrEncodeMode[temp1];
    
    // 计算amr音频数据帧大小
    // 原理: amr 一帧对应20ms，那么一秒有50帧的音频数据
    temp2 = myround((double)(((double)mode / (double)AMR_FRAME_COUNT_PER_SECOND) / (double)8));
    
    frameSize = myround((double)temp2 + 0.5);
    return frameSize;
}

//decode

const int myround(const double x)
{
    return((int)(x+0.5));
}


@end



```

[整个项目的代码在GitHub上的](https://github.com/iWolf/RMCQAudioQueue.git)。可自行下载运行。




<br>
转载请注明：[iWolf的博客](http://iWolf.com) » [局网内点对点语音对讲二](http://iWolf.com/2018/03/局网内点对点语音对讲二/)  


🈳️